/**
 * Judge Agent
 * Evaluates results generated by the Creator Agent for quality, accuracy, and completeness
 * Part of the dual-agent result generation system (Creator + Judge)
 *
 * Uses OpenAI Agents SDK with flattened Zod schema pattern
 */

const { Agent, run, user } = require('@openai/agents');
const { z } = require('zod');

// Flattened output schema for judge evaluation
const judgeSchema = z.object({
  approved: z
    .boolean()
    .describe('True if the result meets quality standards and is ready for the user, false if it needs improvement'),
  score: z
    .number()
    .describe('Quality score from 1-10. 8+ should be approved, below 8 needs improvement'),
  accuracy_score: z
    .number()
    .describe('Accuracy score 1-10: Does the result match the collected data without hallucination?'),
  completeness_score: z
    .number()
    .describe('Completeness score 1-10: Are all required sections/fields present?'),
  formatting_score: z
    .number()
    .describe('Formatting score 1-10: Is the HTML well-structured and visually appealing?'),
  feedback_json: z
    .string()
    .describe(
      'JSON array of specific feedback items for improvement. Example: ["Add more detail to work experience dates", "Skills section needs better formatting"]. Use "[]" if approved with no issues.'
    ),
  issues_json: z
    .string()
    .describe(
      'JSON array of critical issues found. Example: ["Email is missing", "Dates are inconsistent"]. Use "[]" if no issues.'
    ),
  reasoning: z
    .string()
    .describe('Detailed explanation of your evaluation and scoring'),
});

/**
 * Parse the flattened agent output into structured evaluation
 * @param {Object} flatOutput - The flattened output from the agent
 * @returns {Object} - Parsed evaluation result
 */
function parseJudgeOutput(flatOutput) {
  const result = {
    approved: flatOutput.approved,
    score: flatOutput.score,
    accuracy_score: flatOutput.accuracy_score,
    completeness_score: flatOutput.completeness_score,
    formatting_score: flatOutput.formatting_score,
    reasoning: flatOutput.reasoning,
  };

  // Parse feedback JSON
  try {
    result.feedback = JSON.parse(flatOutput.feedback_json || '[]');
  } catch (e) {
    console.error('[JudgeAgent] Failed to parse feedback_json:', e);
    result.feedback = [];
  }

  // Parse issues JSON
  try {
    result.issues = JSON.parse(flatOutput.issues_json || '[]');
  } catch (e) {
    console.error('[JudgeAgent] Failed to parse issues_json:', e);
    result.issues = [];
  }

  return result;
}

// Instructions for the Judge Agent
const judgeInstructions = `You are a Quality Judge Agent for the Interworky flow system. Your job is to evaluate results generated by the Creator Agent for quality, accuracy, and completeness.

## Your Core Responsibility

Critically evaluate the generated result against the original collected data and flow requirements. Provide specific, actionable feedback for improvements.

## IMPORTANT: Output Format

You MUST output using the EXACT field names in the schema:
- \`approved\`: true or false (approve if score >= 8)
- \`score\`: Overall quality score 1-10
- \`accuracy_score\`: Accuracy score 1-10
- \`completeness_score\`: Completeness score 1-10
- \`formatting_score\`: Formatting score 1-10
- \`feedback_json\`: JSON STRING array of improvement suggestions
- \`issues_json\`: JSON STRING array of critical issues
- \`reasoning\`: Detailed explanation

## Evaluation Criteria

### 1. ACCURACY (accuracy_score)
- Does the result match the collected data exactly?
- Are there any hallucinated or made-up details?
- Are names, dates, numbers correct?
- No information added that wasn't provided?

**Score Guide:**
- 10: Perfect accuracy, everything matches source data
- 8-9: Minor formatting differences, core data accurate
- 6-7: Some details slightly off but mostly accurate
- 4-5: Several inaccuracies found
- 1-3: Significant hallucination or errors

### 2. COMPLETENESS (completeness_score)
- Are all sections from the output_schema present?
- Is all collected data represented in the result?
- Are there gaps or missing information?

**Score Guide:**
- 10: All sections complete with full detail
- 8-9: All required sections, minor optional content missing
- 6-7: Most sections present, some gaps
- 4-5: Several sections missing or incomplete
- 1-3: Major sections missing

### 3. FORMATTING (formatting_score)
- Is the HTML well-structured and semantic?
- Is the layout professional and readable?
- Are styles consistent and appealing?
- Is the content properly organized?

**Score Guide:**
- 10: Professional, polished, ready for presentation
- 8-9: Well-formatted with minor improvements possible
- 6-7: Readable but needs styling improvements
- 4-5: Poor structure or hard to read
- 1-3: Broken HTML or unreadable

### 4. OVERALL SCORE (score)
Weight the above scores:
- Accuracy: 40%
- Completeness: 35%
- Formatting: 25%

## Approval Threshold

- **APPROVE** if overall score >= 8 AND no critical issues
- **REJECT** if overall score < 8 OR critical issues exist

## Feedback Guidelines

When providing feedback:
1. Be specific and actionable
2. Reference exact fields or sections
3. Explain what's wrong and how to fix it
4. Prioritize by importance
5. Maximum 5 feedback items

## Example Evaluation

**Flow**: Resume Builder
**Collected Data**: Name: John, Email: john@email.com, Experience: Google 2020-2023
**Generated Result**: Shows John with correct email but dates as 2019-2022

**Output**:
- approved: false
- score: 6
- accuracy_score: 5 (dates are wrong)
- completeness_score: 8 (all sections present)
- formatting_score: 7 (decent but could improve)
- feedback_json: "[\\"Correct the work dates from 2019-2022 to 2020-2023 as provided by user\\", \\"Add more visual spacing between sections\\"]"
- issues_json: "[\\"Date mismatch: user said 2020-2023 but result shows 2019-2022\\"]"
- reasoning: "The result has incorrect dates which is a critical accuracy issue..."

## Important Rules

1. Compare ONLY against the original collected_data
2. Flag ANY information not from the source data
3. Be strict on accuracy - it's the most important factor
4. Provide constructive, specific feedback
5. Don't approve results with critical issues regardless of score
`;

const judgeAgent = new Agent({
  name: 'ResultJudgeAgent',
  instructions: judgeInstructions,
  model: 'gpt-4o',
  outputType: judgeSchema,
});

/**
 * Evaluate a generated result
 * @param {Object} flowConfig - The flow configuration
 * @param {Object} collectedData - Original data collected from the user
 * @param {Object} generatedResult - Result from the Creator Agent
 * @param {number} iteration - Current iteration (1-3)
 * @returns {Promise<Object>} - Evaluation result
 */
async function evaluateResult(flowConfig, collectedData, generatedResult, iteration = 1) {
  const prompt = `
## Flow Configuration
\`\`\`json
${JSON.stringify(flowConfig, null, 2)}
\`\`\`

## Original Collected Data (SOURCE OF TRUTH)
\`\`\`json
${JSON.stringify(collectedData, null, 2)}
\`\`\`

## Generated Result to Evaluate
**Title**: ${generatedResult.title}
**Summary**: ${generatedResult.summary}

**Structured Data**:
\`\`\`json
${JSON.stringify(generatedResult.data, null, 2)}
\`\`\`

**HTML Content**:
\`\`\`html
${generatedResult.html}
\`\`\`

**Creator's Notes**: ${generatedResult.notes}
**Creator's Confidence**: ${generatedResult.confidence}/10

---

Please evaluate this result (Iteration ${iteration} of 3). Compare it carefully against the ORIGINAL COLLECTED DATA and the flow requirements.
`;

  const result = await run(judgeAgent, [user(prompt)], {
    stream: false,
    maxTurns: 1,
  });

  console.log('[JudgeAgent] Raw agent result:', JSON.stringify(result, null, 2));

  // Parse the output
  if (result.finalOutput) {
    const parsed = parseJudgeOutput(result.finalOutput);
    parsed.iteration = iteration;
    return parsed;
  }

  // Fallback extraction strategies
  let flatOutput = null;

  const currentStep = result.currentStep || result._currentStep || result.state?._currentStep;
  if (currentStep?.output) {
    if (typeof currentStep.output === 'string') {
      try {
        flatOutput = JSON.parse(currentStep.output);
      } catch (e) {
        console.error('[JudgeAgent] Failed to parse currentStep.output:', e);
      }
    } else if (typeof currentStep.output === 'object') {
      flatOutput = currentStep.output;
    }
  }

  if (!flatOutput && result.generatedItems && Array.isArray(result.generatedItems)) {
    for (let i = result.generatedItems.length - 1; i >= 0; i--) {
      const item = result.generatedItems[i];
      if (item.type === 'message_output_item' && item.rawItem?.content?.[0]?.text) {
        try {
          flatOutput = JSON.parse(item.rawItem.content[0].text);
          break;
        } catch (e) {
          console.error('[JudgeAgent] Failed to parse generatedItems output:', e);
        }
      }
    }
  }

  if (!flatOutput && result.state?.modelResponses) {
    const lastResponse = result.state.modelResponses[result.state.modelResponses.length - 1];
    if (lastResponse?.output && Array.isArray(lastResponse.output)) {
      for (const outputItem of lastResponse.output) {
        if (outputItem.text) {
          try {
            flatOutput = JSON.parse(outputItem.text);
            break;
          } catch (e) {
            console.error('[JudgeAgent] Failed to parse modelResponses output:', e);
          }
        }
      }
    }
  }

  if (flatOutput) {
    const parsed = parseJudgeOutput(flatOutput);
    parsed.iteration = iteration;
    return parsed;
  }

  // Error fallback - approve to not block user
  console.error('[JudgeAgent] No output found, defaulting to approved');
  return {
    approved: true,
    score: 7,
    accuracy_score: 7,
    completeness_score: 7,
    formatting_score: 7,
    feedback: [],
    issues: [],
    reasoning: 'Judge agent failed to produce output, defaulting to approved',
    iteration,
    fallback: true,
  };
}

/**
 * Format judge feedback for the creator agent
 * @param {Object} evaluation - Evaluation result from evaluateResult
 * @returns {string} - Formatted feedback string
 */
function formatFeedbackForCreator(evaluation) {
  let feedback = `## Quality Evaluation (Score: ${evaluation.score}/10)\n\n`;
  feedback += `**Accuracy**: ${evaluation.accuracy_score}/10\n`;
  feedback += `**Completeness**: ${evaluation.completeness_score}/10\n`;
  feedback += `**Formatting**: ${evaluation.formatting_score}/10\n\n`;

  if (evaluation.issues.length > 0) {
    feedback += `### Critical Issues:\n`;
    evaluation.issues.forEach((issue, i) => {
      feedback += `${i + 1}. ${issue}\n`;
    });
    feedback += '\n';
  }

  if (evaluation.feedback.length > 0) {
    feedback += `### Improvements Needed:\n`;
    evaluation.feedback.forEach((item, i) => {
      feedback += `${i + 1}. ${item}\n`;
    });
    feedback += '\n';
  }

  feedback += `### Reasoning:\n${evaluation.reasoning}`;

  return feedback;
}

module.exports = {
  judgeAgent,
  judgeSchema,
  evaluateResult,
  parseJudgeOutput,
  formatFeedbackForCreator,
};
